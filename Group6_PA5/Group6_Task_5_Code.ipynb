{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing spark modules\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# initializing SparkSession object as session\n",
    "session = SparkSession.builder.appName(\"pipeline\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data \n",
    "df = session.read.csv(\"Housing_data-Final-2-Copy1.csv\",header= True, inferSchema = True,nullValue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+--------+-------+-------+--------+----------+--------+--------------------+--------------+\n",
      "|    Suburb|Type|  Price|Distance|Zipcode|Bedroom|Bathroom|Car_Garage|Lot_size|         Region_name|Property_count|\n",
      "+----------+----+-------+--------+-------+-------+--------+----------+--------+--------------------+--------------+\n",
      "|Abbotsford|   h|1165000|     2.5|   3067|      3|       2|      null|      92|Northern Metropol...|          4019|\n",
      "|Abbotsford|   h|1050000|     2.5|   3067|      2|       1|      null|     129|Northern Metropol...|          4019|\n",
      "|Abbotsford|   h|1465000|     2.5|   3067|      3|       2|      null|     134|Northern Metropol...|          4019|\n",
      "|Abbotsford|   h| 911000|     3.0|   3067|      2|       1|      null|     141|Northern Metropol...|          4019|\n",
      "|Abbotsford|   h|1635000|     3.0|   3067|      3|       1|      null|     142|Northern Metropol...|          4019|\n",
      "+----------+----+-------+--------+-------+-------+--------+----------+--------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the dataframe\n",
    "df = df.withColumnRenamed(\"#Bedroom\",\"Bedroom\").withColumnRenamed(\"#Bathroom\",\"Bathroom\").withColumnRenamed(\"#-Car Garage\",\"Car_Garage\")\n",
    "# checking for negative values and replacing with null value in one feature,if encountered\n",
    "smooth = F.udf(lambda x: x if x > 0 else None, IntegerType())\n",
    "df= df.withColumn('Lot_size', smooth(F.col('Lot_size')))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----------+------------+\n",
      "|Car_Garage|Bathroom|out_garrage|out_Bathroom|\n",
      "+----------+--------+-----------+------------+\n",
      "|      null|       2|          1|           2|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       2|          1|           2|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       2|          1|           2|\n",
      "|      null|       1|          1|           1|\n",
      "|      null|       1|          1|           1|\n",
      "|         1|       2|          1|           2|\n",
      "|         1|       2|          1|           2|\n",
      "|         1|       2|          1|           2|\n",
      "|         1|       2|          1|           2|\n",
      "|         1|       2|          1|           2|\n",
      "+----------+--------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Bedroom: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car_Garage: integer (nullable = true)\n",
      " |-- Lot_size: integer (nullable = true)\n",
      " |-- Region_name: string (nullable = true)\n",
      " |-- Property_count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11579"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing imputer\n",
    "from pyspark.ml.feature import Imputer\n",
    "## filling null values using mean of neighbour values\n",
    "imputer = Imputer(inputCols=[\"Car_Garage\", \"Bathroom\"], outputCols=[\"out_garrage\", \"out_Bathroom\"])\n",
    "\n",
    "## printing the imputed result\n",
    "imputer.fit(df).transform(df).select(\"Car_Garage\", \"Bathroom\",\"out_garrage\",\"out_Bathroom\").show()\n",
    "\n",
    "# schema of dataframe\n",
    "df.printSchema()\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-34-a08d502fdf3f>\", line 4, in <lambda>\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0d9d900ab341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf_pd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/Users/govardhan/opt/anaconda3/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-34-a08d502fdf3f>\", line 4, in <lambda>\nTypeError: '>' not supported between instances of 'NoneType' and 'int'\n"
     ]
    }
   ],
   "source": [
    "## converting string features to vectors by using string indexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "suburb      = StringIndexer(inputCol=\"Suburb\", outputCol=\"Suburb_indexed\",handleInvalid='skip')\n",
    "Type        = StringIndexer(inputCol=\"Type\", outputCol=\"Type_indexed\",handleInvalid='skip')\n",
    "region_name = StringIndexer(inputCol=\"Region_name\", outputCol=\"Region_name_indexed\",handleInvalid='skip')\n",
    "\n",
    "#importing pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "## fitting stages to pipeline\n",
    "pipeline_indexing = Pipeline(stages=[suburb, Type, region_name])\n",
    "\n",
    "## transforming dataframe by using pipeline\n",
    "df_=pipeline_indexing.fit(df).transform(df)\n",
    "\n",
    "## importing pandas\n",
    "import pandas\n",
    "\n",
    "df_pd = df_.toPandas()\n",
    "\n",
    "df_pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding correlation with target Label \"Price\"\n",
    "Distance_corr=df_pd['Distance'].corr(df_pd['Price'])\n",
    "Zipcode_corr=df_pd['Zipcode'].corr(df_pd['Price'])\n",
    "Bedroom_corr=df_pd['Bedroom'].corr(df_pd['Price'])\n",
    "Bathroom_corr=df_pd['Bathroom'].corr(df_pd['Price'])\n",
    "Car_Garage_corr=df_pd['Car_Garage'].corr(df_pd['Price'])\n",
    "Lot_size_corr=df_pd['Lot_size'].corr(df_pd['Price'])\n",
    "Property_count_corr=df_pd['Property_count'].corr(df_pd['Price'])\n",
    "Suburb_indexed_corr=df_pd['Suburb_indexed'].corr(df_pd['Price'])\n",
    "Type_indexed_corr=df_pd['Type_indexed'].corr(df_pd['Price'])\n",
    "Region_name_indexed_corr=df_pd['Region_name_indexed'].corr(df_pd['Price'])\n",
    "Lot_size_corr=df_pd['Lot_size'].corr(df_pd['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distance correlation is   :\",Distance_corr)\n",
    "print(\"Zipcode correlation is    :\",Zipcode_corr)\n",
    "print(\"Bedroom correlation is    :\",Bedroom_corr)\n",
    "print(\"Bathroom correlation is   :\",Bathroom_corr)\n",
    "print(\"Car_Garage correlation is :\",Car_Garage_corr)\n",
    "print(\"Lot_size correlation is   :\",Lot_size_corr)\n",
    "print(\"Property correlation is            :\",Property_count_corr)\n",
    "print(\"Suburb_indexed correlation is      :\",Suburb_indexed_corr)\n",
    "print(\"Type_indexed correlation is        :\",Type_indexed_corr)\n",
    "print(\"Region_name_indexed correlation is :\",Region_name_indexed_corr)\n",
    "print(\"Lot_size correlation is            :\",Lot_size_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK_3(a)training with highest correlation features:\n",
    "\n",
    "# importing Spark ML modules\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Normalizer\n",
    "\n",
    "# Assebling features into a vector matrix\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"Bedroom\", \"out_Bathroom\",\"Zipcode\"],\n",
    "    outputCol=\"features\")\n",
    "assembler.setParams(handleInvalid=\"skip\")\n",
    "\n",
    "# Normalizing the vector of features for model improvement\n",
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2)\n",
    "\n",
    "# Splitting data inti test and train sets\n",
    "(trainingData, testData) = df.randomSplit([0.8, 0.2],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of train records are\",trainingData.count())\n",
    "print(\"Number of test records are\",testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training  a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"normFeatures\",labelCol='Price',)\n",
    "\n",
    "## Task Pipeline\n",
    "# fitting the above stages in pipeline\n",
    "pipeline = Pipeline(stages=[imputer,pipeline_indexing,assembler,normalizer, rf])\n",
    "\n",
    "# Training model. \n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Making predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "## showing the predictions and actual values\n",
    "predictions.select(\"prediction\", \"Price\").show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK-4 RMSE Metric on predicting Label \"Price\" with highest correlation features \n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK-3 (b)Predicting label \"Price\" using all features\n",
    "## Assembling all features into a vector matric as features\n",
    "\n",
    "assembler1 = VectorAssembler(\n",
    "    inputCols=[\n",
    " 'Distance',\n",
    " 'Zipcode',\n",
    " 'Bedroom',\n",
    "        \"out_garrage\",\"out_Bathroom\",\n",
    " 'Lot_size',\n",
    " 'Property_count'],\n",
    "    outputCol=\"features\")\n",
    "assembler1.setParams(handleInvalid=\"skip\")\n",
    "\n",
    "### Normalizing the vector of features for model improvement\n",
    "normalizer1 = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=2)\n",
    "\n",
    "## Spliting the data into test and train sets\n",
    "(trainingData1, testData1) = df.randomSplit([0.8, 0.2],100)\n",
    "\n",
    "\n",
    "# Training a RandomForest model.\n",
    "rf1 = RandomForestRegressor(featuresCol=\"normFeatures\",labelCol='Price')\n",
    "\n",
    "# pipeline for the stages above\n",
    "pipeline1 = Pipeline(stages=[imputer,pipeline_indexing,assembler1,normalizer1, rf1])\n",
    "\n",
    "# Training the model\n",
    "model1 = pipeline1.fit(trainingData1)\n",
    "\n",
    "# Making predictions.\n",
    "predictions1 = model1.transform(testData1)\n",
    "\n",
    "predictions1.select(\"prediction\", \"Price\",\"normFeatures\").show(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK-4 RMSE Metric on predicting Label \"Price\" with all features \n",
    "evaluator1 = RegressionEvaluator(\n",
    "    labelCol=\"Price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse1 = evaluator1.evaluate(predictions1)\n",
    "\n",
    "print(rmse1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
